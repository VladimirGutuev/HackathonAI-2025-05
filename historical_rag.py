import os
import requests
import json
import sqlite3
import numpy as np
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pickle
import hashlib

class HistoricalRAG:
    """
    –°–∏—Å—Ç–µ–º–∞ RAG (Retrieval-Augmented Generation) –¥–ª—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Ç–∫—Ä—ã—Ç—ã–µ API Wikipedia –∏ Historical Events –¥–ª—è –æ–±–æ–≥–∞—â–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞ –≤–æ–µ–Ω–Ω—ã—Ö –¥–Ω–µ–≤–Ω–∏–∫–æ–≤.
    """
    
    def __init__(self, db_path: str = "historical_rag.db", cache_dir: str = "rag_cache"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã RAG
        
        Args:
            db_path: –ü—É—Ç—å –∫ SQLite –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
            cache_dir: –ü–∞–ø–∫–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤
        """
        self.db_path = db_path
        self.cache_dir = cache_dir
        
        # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –¥–ª—è –∫—ç—à–∞ –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        os.makedirs(cache_dir, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        self._init_database()
        
        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –¥–ª—è semantic search
        self.vectorizer = TfidfVectorizer(
            max_features=10000,
            stop_words='english',
            ngram_range=(1, 2),
            min_df=2
        )
        
        # –ö—ç—à –¥–ª—è –≤–µ–∫—Ç–æ—Ä–æ–≤
        self.vector_cache_path = os.path.join(cache_dir, "vectors.pkl")
        self.texts_cache_path = os.path.join(cache_dir, "texts.pkl")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã –∏–∑ –∫—ç—à–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
        self._load_vectors_cache()
        
        # API —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã
        self.wikipedia_api = "https://ru.wikipedia.org/w/api.php"
        self.historical_events_api = "http://www.vizgr.org/historical-events/search.php"
        
        print("HistoricalRAG –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def _init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SQLite –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–∫—Ç–æ–≤"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–æ–±—ã—Ç–∏–π
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS historical_events (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                date TEXT,
                title TEXT,
                description TEXT,
                source TEXT,
                category TEXT,
                relevance_score REAL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                hash TEXT UNIQUE
            )
        ''')
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è Wikipedia —Å—Ç–∞—Ç–µ–π
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS wikipedia_articles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT,
                content TEXT,
                url TEXT,
                extract TEXT,
                categories TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                hash TEXT UNIQUE
            )
        ''')
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è RAG —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–∫—ç—à)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS rag_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                query_hash TEXT UNIQUE,
                query_text TEXT,
                results TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_events_date ON historical_events(date)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_events_category ON historical_events(category)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_wiki_title ON wikipedia_articles(title)')
        
        conn.commit()
        conn.close()

    def _load_vectors_cache(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏–∑ –∫—ç—à–∞"""
        self.vectors = None
        self.cached_texts = []
        
        if os.path.exists(self.vector_cache_path) and os.path.exists(self.texts_cache_path):
            try:
                with open(self.vector_cache_path, 'rb') as f:
                    self.vectors = pickle.load(f)
                with open(self.texts_cache_path, 'rb') as f:
                    self.cached_texts = pickle.load(f)
                print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.cached_texts)} —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ –∫—ç—à–∞")
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {e}")
                self.vectors = None
                self.cached_texts = []

    def _save_vectors_cache(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –∫—ç—à"""
        try:
            with open(self.vector_cache_path, 'wb') as f:
                pickle.dump(self.vectors, f)
            with open(self.texts_cache_path, 'wb') as f:
                pickle.dump(self.cached_texts, f)
            print("–í–µ–∫—Ç–æ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –∫—ç—à")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞: {e}")

    def extract_dates_from_text(self, text: str) -> List[Tuple[str, str]]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–Ω–µ–≤–Ω–∏–∫–∞
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–Ω–∞–π–¥–µ–Ω–Ω–∞—è_–¥–∞—Ç–∞, –∫–æ–Ω—Ç–µ–∫—Å—Ç)
        """
        date_patterns = [
            # –†—É—Å—Å–∫–∏–µ –¥–∞—Ç—ã
            r'(\d{1,2})\s+(—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+(\d{4})',
            r'(\d{1,2})\.(\d{1,2})\.(\d{4})',
            r'(\d{4})\s*–≥\.?',
            # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –¥–∞—Ç—ã
            r'(January|February|March|April|May|June|July|August|September|October|November|December)\s+(\d{1,2}),?\s+(\d{4})',
            r'(\d{1,2})\s+(January|February|March|April|May|June|July|August|September|October|November|December)\s+(\d{4})',
        ]
        
        dates = []
        for pattern in date_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                date_str = match.group(0)
                # –ö–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –¥–∞—Ç—ã (50 —Å–∏–º–≤–æ–ª–æ–≤ –¥–æ –∏ –ø–æ—Å–ª–µ)
                start = max(0, match.start() - 50)
                end = min(len(text), match.end() + 50)
                context = text[start:end].strip()
                dates.append((date_str, context))
        
        return dates

    def extract_historical_keywords(self, text: str, emotion_analysis: Dict = None) -> List[str]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            emotion_analysis: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
        """
        keywords = []
        
        # –í–æ–µ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        military_terms = [
            '–≤–æ–π–Ω–∞', '–±–∏—Ç–≤–∞', '—Å—Ä–∞–∂–µ–Ω–∏–µ', '—Ñ—Ä–æ–Ω—Ç', '–∞—Ç–∞–∫–∞', '–æ–±–æ—Ä–æ–Ω–∞', '–Ω–∞—Å—Ç—É–ø–ª–µ–Ω–∏–µ',
            '—Ç–∞–Ω–∫', '—Å–∞–º–æ–ª—ë—Ç', '–∞—Ä—Ç–∏–ª–ª–µ—Ä–∏—è', '–≤–∏–Ω—Ç–æ–≤–∫–∞', '—Ç—Ä–∞–Ω—à–µ—è', '–±–ª–∏–Ω–¥–∞–∂',
            '–∫–æ–º–∞–Ω–¥–∏—Ä', '—Å–æ–ª–¥–∞—Ç', '–æ—Ñ–∏—Ü–µ—Ä', '–≥–µ–Ω–µ—Ä–∞–ª', '–ø–æ–ª–∫', '–¥–∏–≤–∏–∑–∏—è', '–∞—Ä–º–∏—è',
            'war', 'battle', 'front', 'attack', 'defense', 'tank', 'aircraft', 'artillery'
        ]
        
        # –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –æ–±—ä–µ–∫—Ç—ã
        geo_terms = [
            '–ú–æ—Å–∫–≤–∞', '–õ–µ–Ω–∏–Ω–≥—Ä–∞–¥', '–°—Ç–∞–ª–∏–Ω–≥—Ä–∞–¥', '–ö—É—Ä—Å–∫', '–ë–µ—Ä–ª–∏–Ω', '–í–∞—Ä—à–∞–≤–∞',
            'Moscow', 'Leningrad', 'Stalingrad', 'Kursk', 'Berlin', 'Warsaw'
        ]
        
        # –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –ø–µ—Ä–∏–æ–¥—ã
        historical_periods = [
            '–í–µ–ª–∏–∫–∞—è –û—Ç–µ—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –≤–æ–π–Ω–∞', '–í—Ç–æ—Ä–∞—è –º–∏—Ä–æ–≤–∞—è –≤–æ–π–Ω–∞', '–ü–µ—Ä–≤–∞—è –º–∏—Ä–æ–≤–∞—è –≤–æ–π–Ω–∞',
            'World War II', 'World War I', 'WWII', 'WWI', '1941', '1942', '1943', '1944', '1945'
        ]
        
        all_terms = military_terms + geo_terms + historical_periods
        
        # –ò—â–µ–º —Ç–µ—Ä–º–∏–Ω—ã –≤ —Ç–µ–∫—Å—Ç–µ
        text_lower = text.lower()
        for term in all_terms:
            if term.lower() in text_lower:
                keywords.append(term)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        if emotion_analysis and 'thematic_analysis' in emotion_analysis:
            thematic = emotion_analysis['thematic_analysis']
            for category in ['military_characters', 'battle_locations', 'war_equipment', 'historical_events']:
                if category in thematic:
                    keywords.extend(thematic[category])
        
        return list(set(keywords))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã

    def search_wikipedia(self, query: str, limit: int = 5) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ –≤ Wikipedia API
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π Wikipedia
        """
        try:
            # –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π
            search_params = {
                'action': 'opensearch',
                'search': query,
                'limit': limit,
                'namespace': 0,
                'format': 'json'
            }
            
            search_response = requests.get(self.wikipedia_api, params=search_params, timeout=10)
            search_data = search_response.json()
            
            if len(search_data) < 4:
                return []
            
            titles = search_data[1]
            descriptions = search_data[2]
            urls = search_data[3]
            
            articles = []
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞–∂–¥–æ–π —Å—Ç–∞—Ç—å–µ
            for i, title in enumerate(titles):
                article_params = {
                    'action': 'query',
                    'titles': title,
                    'prop': 'extracts|categories',
                    'exintro': True,
                    'explaintext': True,
                    'exsectionformat': 'plain',
                    'format': 'json'
                }
                
                article_response = requests.get(self.wikipedia_api, params=article_params, timeout=10)
                article_data = article_response.json()
                
                if 'query' in article_data and 'pages' in article_data['query']:
                    page_id = list(article_data['query']['pages'].keys())[0]
                    page_data = article_data['query']['pages'][page_id]
                    
                    extract = page_data.get('extract', descriptions[i] if i < len(descriptions) else '')
                    categories = [cat['title'] for cat in page_data.get('categories', [])]
                    
                    article = {
                        'title': title,
                        'extract': extract,
                        'url': urls[i] if i < len(urls) else '',
                        'categories': categories,
                        'source': 'wikipedia'
                    }
                    
                    articles.append(article)
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
                    self._save_wikipedia_article(article)
            
            return articles
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ Wikipedia: {e}")
            return []

    def search_historical_events(self, query: str = None, begin_date: str = None, 
                                end_date: str = None, limit: int = 10) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–æ–±—ã—Ç–∏–π
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            begin_date: –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ (YYYYMMDD)
            end_date: –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ (YYYYMMDD)
            limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–æ–±—ã—Ç–∏–π
        """
        try:
            params = {
                'format': 'json',
                'limit': limit,
                'lang': 'en'  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
            }
            
            if query:
                params['query'] = query
            if begin_date:
                params['begin_date'] = begin_date
            if end_date:
                params['end_date'] = end_date
            
            response = requests.get(self.historical_events_api, params=params, timeout=15)
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    events = []
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –æ—Ç–≤–µ—Ç–∞ API
                    if isinstance(data, dict) and 'result' in data:
                        for event in data['result']:
                            if isinstance(event, dict):
                                event_data = {
                                    'date': event.get('date', ''),
                                    'title': event.get('title', ''),
                                    'description': event.get('description', ''),
                                    'category': event.get('category', ''),
                                    'source': 'historical_events_api'
                                }
                                events.append(event_data)
                                
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
                                self._save_historical_event(event_data)
                    elif isinstance(data, list):
                        # –ï—Å–ª–∏ API –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–∞—Å—Å–∏–≤ –Ω–∞–ø—Ä—è–º—É—é
                        for event in data:
                            if isinstance(event, dict):
                                event_data = {
                                    'date': event.get('date', ''),
                                    'title': event.get('title', ''),
                                    'description': event.get('description', ''),
                                    'category': event.get('category', ''),
                                    'source': 'historical_events_api'
                                }
                                events.append(event_data)
                                
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
                                self._save_historical_event(event_data)
                    
                    return events
                    
                except json.JSONDecodeError:
                    # API –≤–µ—Ä–Ω—É–ª –Ω–µ JSON, –≤–æ–∑–º–æ–∂–Ω–æ XML –∏–ª–∏ HTML
                    print("API –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–æ–±—ã—Ç–∏–π –≤–µ—Ä–Ω—É–ª –Ω–µ JSON —Ñ–æ—Ä–º–∞—Ç")
                    return []
            else:
                print(f"–û—à–∏–±–∫–∞ API –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–æ–±—ã—Ç–∏–π: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Å–æ–±—ã—Ç–∏–π: {e}")
            return []

    def _save_wikipedia_article(self, article: Dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏ Wikipedia –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –°–æ–∑–¥–∞—ë–º —Ö—ç—à –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            content_hash = hashlib.md5(
                f"{article['title']}{article['extract']}".encode()
            ).hexdigest()
            
            cursor.execute('''
                INSERT OR IGNORE INTO wikipedia_articles 
                (title, content, url, extract, categories, hash)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                article['title'],
                article['extract'],
                article['url'],
                article['extract'],
                json.dumps(article['categories']),
                content_hash
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è Wikipedia —Å—Ç–∞—Ç—å–∏: {e}")

    def _save_historical_event(self, event: Dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–±—ã—Ç–∏—è –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –°–æ–∑–¥–∞—ë–º —Ö—ç—à –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            content_hash = hashlib.md5(
                f"{event['date']}{event['title']}{event['description']}".encode()
            ).hexdigest()
            
            cursor.execute('''
                INSERT OR IGNORE INTO historical_events 
                (date, title, description, source, category, hash)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                event['date'],
                event['title'],
                event['description'],
                event['source'],
                event['category'],
                content_hash
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–±—ã—Ç–∏—è: {e}")

    def get_relevant_historical_context(self, diary_text: str, emotion_analysis: Dict = None, 
                                      top_k: int = 5) -> List[Dict]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ –¥–Ω–µ–≤–Ω–∏–∫–∞
        
        Args:
            diary_text: –¢–µ–∫—Å—Ç –¥–Ω–µ–≤–Ω–∏–∫–∞
            emotion_analysis: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–∫—Ç–æ–≤
        """
        print("–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞...")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ –¥–∞—Ç—ã
        keywords = self.extract_historical_keywords(diary_text, emotion_analysis)
        dates = self.extract_dates_from_text(diary_text)
        
        print(f"–ù–∞–π–¥–µ–Ω—ã –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keywords[:5]}")  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
        print(f"–ù–∞–π–¥–µ–Ω—ã –¥–∞—Ç—ã: {[d[0] for d in dates[:3]]}")  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
        
        all_context = []
        
        # –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        for keyword in keywords[:3]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤
            # Wikipedia
            wiki_results = self.search_wikipedia(keyword, limit=2)
            all_context.extend(wiki_results)
            
            # –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Å–æ–±—ã—Ç–∏—è
            historical_results = self.search_historical_events(query=keyword, limit=3)
            all_context.extend(historical_results)
        
        # –ü–æ–∏—Å–∫ –ø–æ –¥–∞—Ç–∞–º
        for date_str, context in dates[:2]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞—Ç
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ–±—ã—Ç–∏–π
                year_match = re.search(r'(\d{4})', date_str)
                if year_match:
                    year = year_match.group(1)
                    begin_date = f"{year}0101"
                    end_date = f"{year}1231"
                    
                    date_events = self.search_historical_events(
                        begin_date=begin_date, 
                        end_date=end_date, 
                        limit=3
                    )
                    all_context.extend(date_events)
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞—Ç—ã {date_str}: {e}")
        
        # –†–∞–Ω–∂–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        if all_context:
            ranked_context = self._rank_context_by_relevance(diary_text, all_context)
            return ranked_context[:top_k]
        
        return []

    def get_historical_context_dict(self, diary_text: str, emotion_analysis: Dict = None, 
                                   top_k: int = 5) -> Dict:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ —Å–ª–æ–≤–∞—Ä—è
        
        Args:
            diary_text: –¢–µ–∫—Å—Ç –¥–Ω–µ–≤–Ω–∏–∫–∞
            emotion_analysis: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
        """
        context_items = self.get_relevant_historical_context(diary_text, emotion_analysis, top_k)
        
        if context_items:
            context_type = 'full_context'
            if len(context_items) < 3:
                context_type = 'limited_context'
        else:
            context_type = 'no_context'
        
        return {
            'found_items': len(context_items),
            'context_items': context_items,
            'context_type': context_type,
            'summary': self.generate_historical_summary(context_items) if context_items else "–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω."
        }

    def _rank_context_by_relevance(self, query_text: str, context_items: List[Dict]) -> List[Dict]:
        """
        –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        
        Args:
            query_text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–Ω–µ–≤–Ω–∏–∫–∞
            context_items: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–∫—Ç–æ–≤
            
        Returns:
            –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Å–ø–∏—Å–æ–∫
        """
        if not context_items:
            return []
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
            context_texts = []
            for item in context_items:
                if 'extract' in item:
                    text = f"{item['title']} {item['extract']}"
                elif 'description' in item:
                    text = f"{item['title']} {item['description']}"
                else:
                    text = item.get('title', '')
                context_texts.append(text)
            
            if not context_texts:
                return context_items
            
            # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç—ã
            all_texts = [query_text] + context_texts
            
            # –ï—Å–ª–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –Ω–µ –æ–±—É—á–µ–Ω, –æ–±—É—á–∞–µ–º –µ–≥–æ
            if self.vectors is None:
                vectors = self.vectorizer.fit_transform(all_texts)
            else:
                vectors = self.vectorizer.transform(all_texts)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
            query_vector = vectors[0:1]
            context_vectors = vectors[1:]
            
            similarities = cosine_similarity(query_vector, context_vectors)[0]
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏
            ranked_indices = np.argsort(similarities)[::-1]
            
            ranked_context = []
            for idx in ranked_indices:
                item = context_items[idx].copy()
                item['relevance_score'] = float(similarities[idx])
                ranked_context.append(item)
            
            return ranked_context
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            return context_items

    def generate_historical_summary(self, context_items: List[Dict]) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—Ä–∞—Ç–∫–æ–≥–æ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–≥–æ —Ä–µ–∑—é–º–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        
        Args:
            context_items: –°–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–∫—Ç–æ–≤
            
        Returns:
            –ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        """
        if not context_items:
            return "–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω."
        
        summary_parts = []
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        wiki_items = [item for item in context_items if item.get('source') == 'wikipedia']
        events_items = [item for item in context_items if item.get('source') == 'historical_events_api']
        
        if wiki_items:
            summary_parts.append("üìö **–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è —Å–ø—Ä–∞–≤–∫–∞ –∏–∑ Wikipedia:**")
            for item in wiki_items[:2]:  # –ë–µ—Ä—ë–º —Ç–æ–ø-2
                title = item.get('title', '')
                extract = item.get('extract', '')[:200] + "..." if len(item.get('extract', '')) > 200 else item.get('extract', '')
                summary_parts.append(f"‚Ä¢ **{title}**: {extract}")
        
        if events_items:
            summary_parts.append("\nüìÖ **–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Å–æ–±—ã—Ç–∏—è:**")
            for item in events_items[:3]:  # –ë–µ—Ä—ë–º —Ç–æ–ø-3
                date = item.get('date', '')
                title = item.get('title', '')
                description = item.get('description', '')[:150] + "..." if len(item.get('description', '')) > 150 else item.get('description', '')
                summary_parts.append(f"‚Ä¢ **{date}**: {title} - {description}")
        
        return "\n".join(summary_parts)

    def enhance_analysis_with_context(self, analysis_result: Dict, historical_context: List[Dict]) -> Dict:
        """
        –û–±–æ–≥–∞—â–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        
        Args:
            analysis_result: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –¥–Ω–µ–≤–Ω–∏–∫–∞
            historical_context: –ù–∞–π–¥–µ–Ω–Ω—ã–π –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            
        Returns:
            –û–±–æ–≥–∞—â—ë–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        """
        enhanced_result = analysis_result.copy()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        enhanced_result['historical_context'] = {
            'found_items': len(historical_context),
            'context_items': historical_context,
            'summary': self.generate_historical_summary(historical_context)
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–ª–∞–≥ –æ –Ω–∞–ª–∏—á–∏–∏ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–≥–æ –æ–±–æ–≥–∞—â–µ–Ω–∏—è
        enhanced_result['has_historical_enrichment'] = len(historical_context) > 0
        
        return enhanced_result

    def clear_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤"""
        try:
            if os.path.exists(self.vector_cache_path):
                os.remove(self.vector_cache_path)
            if os.path.exists(self.texts_cache_path):
                os.remove(self.texts_cache_path)
            self.vectors = None
            self.cached_texts = []
            print("–ö—ç—à –æ—á–∏—â–µ–Ω")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞: {e}")

    def get_database_stats(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("SELECT COUNT(*) FROM historical_events")
            events_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM wikipedia_articles")
            articles_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM rag_results")
            cache_count = cursor.fetchone()[0]
            
            conn.close()
            
            return {
                'historical_events': events_count,
                'wikipedia_articles': articles_count,
                'cached_results': cache_count,
                'vector_cache_exists': os.path.exists(self.vector_cache_path)
            }
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {} 